Here are 50 PySpark interview questions for a Data Engineer (2–3 years experience). These focus on practical concepts, performance, and real-world scenarios commonly asked in interviews.

PySpark Interview Questions (50) – Data Engineer (2–3 Years)
1. PySpark Fundamentals (1–5)

What is PySpark and how does it differ from Apache Spark?

Explain the architecture of Spark (Driver, Executor, Cluster Manager).

What is a SparkSession?

Difference between SparkContext and SparkSession.

What is lazy evaluation in PySpark?

2. RDD vs DataFrame vs Dataset (6–10)

What is an RDD?

Difference between RDD and DataFrame.

Why are DataFrames preferred over RDDs?

What is a Dataset? Is it available in PySpark?

When would you use RDD instead of DataFrame?

3. Transformations & Actions (11–15)

What are transformations in PySpark? Give examples.

What are actions? Give examples.

Difference between narrow and wide transformations.

What happens internally when an action is triggered?

How does Spark build a DAG?

4. DataFrame Operations (16–22)

How do you read data from different sources (CSV, JSON, Parquet)?

How do you filter records in a DataFrame?

Difference between select() and selectExpr().

How do you handle null values?

How do you rename columns?

How do you add a new column?

How do you remove duplicate records?

5. Joins & Aggregations (23–28)

Explain different types of joins in PySpark.

How do you perform a broadcast join?

How do you handle skewed joins?

How do you perform groupBy and aggregation?

Difference between groupBy() and window functions.

How do you find top N records per group?

6. Window Functions (29–32)

What are window functions in PySpark?

Explain row_number(), rank(), and dense_rank().

How do you calculate running totals?

How do you use lag() and lead()?

7. Performance Optimization (33–40)

What is partitioning in PySpark?

Difference between repartition() and coalesce().

What is caching and when should it be used?

Difference between cache() and persist().

What are common causes of slow Spark jobs?

What is data skew and how do you fix it?

What is predicate pushdown?

What is Adaptive Query Execution (AQE)?

8. File Formats & Storage (41–44)

Why is Parquet preferred in PySpark?

Difference between Parquet and CSV.

How do you write partitioned data?

What are small file problems and how do you fix them?

9. Error Handling & Debugging (45–47)

How do you handle errors in PySpark jobs?

How do you debug a failed Spark job?

How do you monitor Spark applications?

10. Real-Time & Data Engineering Scenarios (48–50)

How do you implement incremental data processing in PySpark?

How do you process large datasets efficiently (TB scale)?

How would you design an end-to-end ETL pipeline using PySpark?
